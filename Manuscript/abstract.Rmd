
__Abstract__

Longitudinal studies of development and disease in the human brain
have motivated the acquisition of large neuroimaging data sets and the
concomitant development of robust methodological and statistical tools
for quantifying neurostructural changes.  Longitudinal-specific strategies
for acquisition and processing have potentially significant benefits including
more consistent estimates of intra-subject measurements while retaining predictive power.
In this work, we introduce the open-source Advanced Normalization Tools
(ANTs) cortical thickness longitudinal processing pipeline and its application
on the first phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI-1)
comprising over 600 subjects with multiple time points from baseline to 36 months.
We demonstrate \textcolor{red}{in these data} that the single-subject template
construction and native subject-space processing advantageously localizes data
transformations and mitigates interpolation artifacts \textcolor{red}{which results
in a simultaneous minimization of within-subject variability and maximization of
between-subject variability immediately estimable from a longitudinal mixed-effects
modeling strategy.}  It is further shown that optimizing these dual criteria
leads to greater scientific interpretability in terms of tighter confidence intervals
in calculated mean trends, smaller prediction intervals, and \textcolor{red}{narrower}
confidence intervals for determining cross-sectional effects.  \textcolor{red}{These concepts are
first illustrated and explored in the entorhinal cortex.  This
evaluation strategy is then extended to the entire cortex, as defined by the
Desikan-Killiany-Tourville labeling protocol, where comparisons are made with
the popular cross-sectional and longitudinal FreeSurfer processing streams.}

_Keywords:_  Advanced Normalization Tools, \textcolor{red}{entorhinal cortex},
\textcolor{red}{FreeSurfer}, interpolation,
\textcolor{red}{longitudinal mixed-effects}, longitudinal processing

\clearpage
