
# Materials and Methods

## Imaging

## Cortical thickness

### Cross-sectional processing

In [@Tustison:2014ab] we introduced the ANTs cortical thickness processing
pipeline using a large cohort of $\sim$ 1200 images taken from four popular, publicly
available data sets with ages ranging from 4 to 97 years.  The processing pipeline
comprises the following four major steps:

* N4 bias correction [@Tustison:2010ac],
* skull stripping [@avants2010a],
* $n$-tissue segmentation [@Avants:2011aa], and
* cortical thickness estimation [@das2009]

which is enhanced by the use of optimal shape and intensity templates derived
from the specific populations of study.  Regional statistics were quantified by
parcellating the cortex using a collection of 20 atlases which were labeled using
the Desikan-Killiany-Tourville (DKT) protocol [@Klein:2012aa].  Consensus labelings
in each subject were generated from the joint-label fusion approach of [@Wang:2013ab].
An thickness-based evaluation with the well-known FreeSurfer algorithm demonstrated
better predictive performance of age and gender.  Since the original publication, we
have also added an ANTs implementation of the patch-based denoising algorithm of
[@Manjon:2010aa] as an option.
The resulting regional statistics (including cortical thickness, surface area [@Lehmann:2012aa],
volumes, and Jacobian determinant values) were posted online for public availability
(https://github.com/ntustison/KapowskiChronicles).  These also include the corresponding
FreeSurfer measurements which are also available for public consumption [@Hasan:2016aa].

Although this work was published only recently, it has since been used in a number of studies
[@Price:2015aa;@Wisse:2015aa;@Betancourt:2015aa].





### Longitudinal processing

The ANTs longitudinal cortical thickness pipeline extends the ANTs cortical thickness pipeline
to unbiased longitudinal studies. The pipeline first creates a shape and appearance average
subject-specific template (SST) for each individual.  It then rigidly aligns each time point to the SST to reduce the
effect of coordinate system or interpolation bias.  Subsequent processing segments
the SST into six probabilistic tissues classes:   cerebrospinal
fluid (CSF), gray matter (GM), white matter (WM), deep gray matter (striatum + thalamus),
brain stem, and cerebellum.  This requires processing the SST through two parallel workflows.  First,
the SST proceeds through the standard ANTs cortical thickness pipeline which generates
a brain extraction mask and the CSF posterior probability map.  Second, using
a data set of expert annotations [@Klein:2012aa], a class-leading multi-atlas joint label fusion step [@Wang:2013ab] is performed to
create individualized probability maps for all tissue types.  This final version of the SST enables
unbiased mappings to the group template, subject-specific tissue segmentations, region of interest volumes and
cortical thickness maps for each of the original time series images.
The corresponding cortical labelings (generated
using a multi-atlas label fusion approach and a selected cortical parcellation protocol)
are then used to tabulate regional thickness and area values for statistical analysis.
Other modalities are then mapped to the group template through these unbiased
transformations, as in [@Tustison:2014aa;@Avants:2015aa]

<!--

\input{sampleTexTable.tex}

An equation:

$$ F_1 =  \frac{ 2 \cdot TP }{ 2 \cdot TP + FP + FN}. $$

![Oh, and here is an image of the longitudinal pipeline with a caption and a reference
to the KellyKapowski paper [@Tustison:2014ab].](../Figures/longitudinalPipeline.png)

Here is an example footnote.[^1]

[^1]: \textcolor{blue}{For comparison, the training data set of
the MS Lesion Segmentation challenge
associated with the international MICCAI 2008 conference has a mean lesion load of
204 ($\pm$ 752) mm$^3$ per lesion and the resolution is almost twice what is used
in this study (i.e., 0.5 $\times$ 0.5 $\times$ 0.5).}

-->

## Statistical methods

We used a simple statistical principle to compare performance between cross-sectional and longitudinal processing methods.  We said that one method outperforms the other when it does a better job minimizing within-subject variability and maximizing between-subject variability in cortical thickness measurements.  Such a quality implies greater within-subject reproducibility while distinguishing between patient subpopulations. As such this will amount to higher precision when cortical thickness is used as a predictor variable or model covariate in statistical analyses upstream. This criterion is immediately assessable in terms of estimates associated to the following longitudinal mixed-effects model.

As previously noted we observed yearly cortical thickness measurements from sixty-two separate regions of interest.  To assess the above variance criterion while accounting for changes that may occur through the passage of time, we used a hierarchical Bayesian model for parameter estimation.  Let $Y^k_{ij}$ denote the $i^{th}$ individual's cortical thickness measurement corresponding to the $k^{th}$ region of interest at measurement $j$.  Under the Bayesian paradigm we utilized a model of the form
\begin{gather}
Y^k_{ij} \sim N(\alpha^k_i + \beta^k t, \sigma_k^2) \\ \nonumber
\alpha^k_i \sim N(\alpha^k_0, \tau^2_k) \qquad  \alpha^k_0, \beta^k \sim N(0,10)  \qquad \sigma_k^2,  \tau_k^2 \sim \mbox{Cauchy}^+ (0, 5)
\end{gather}
Specification of parameters in the above prior distributions reflect commonly accepted diffuse priors. $\tau^2_k$ represents the between-subject variance parameter, and $\sigma^2_k$ represents the within-subject variance parameter.  For each region, the quantity of interest is thus the ratio $r^k = \frac{\tau^2_k}{\sigma^2_k}$.  This ratio is closely related to the intraclass correlation coefficient CITE.  The posterior distribution of $r^k$ was summarized via the posterior median. Where the posterior distributions were obtained using stan probabilistic programming language. CITE

For each processing method we performed sixty-two independent regressions.  In order to compare results between methods, we considered the quantity $\delta^k = r^k_l - r^k_c$ and $\delta^k_{norm} = \frac{r^k_l - r^k_c}{r^k_l + r^k_c}$, denoting the variance ratio for the longitudinal method minus that of the cross-sectional method and the normed difference between ratios, respectively. Since a large $r^k$ implies a higher between-subject to within-subject variability ratio, a positive estimate of $\delta^k$ that is large in magnitude implies that the longitudinal processing method is preferable to the cross-sectional method.  Conversely, a negative estimate that is large in magnitude implies that the cross-sectional processing method is preferable to the longitudinal method.





