---
output:
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
    keep_tex: yes
    number_sections: true
  word_document:
    fig_caption: true
classoption: table
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.5}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography:
  - references.bib
  - referencesBA.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 12pt
mainfont: Georgia
geometry: margin=1.in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( cache=TRUE )
```

<!--
\pagenumbering{gobble}
-->

<!--
---
output:
  word_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
    keep_tex: yes
  html_document:
    toc: false
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.2}
   - \usepackage[compact]{titlesec}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=1.0in
---
-->

\begin{centering}

$ $

\vspace{0.1 cm}

\LARGE

{\bf The ANTs Longitudinal Cortical Thickness Pipeline}

\vspace{0.2 cm}

\normalsize

Nicholas J. Tustison$^{1,2}$,
Andrew J. Holbrook$^3$,
Brian B. Avants$^{4\dagger}$,
Jared M. Roberts$^2$,
Philip A. Cook$^4$,
Zachariah M. Reagh$^2$,
Jeffrey T. Duda$^4$,
James R. Stone$^1$,
Daniel L. Gillen$^3$, and
Michael A. Yassa$^2$
for the Alzheimer's Disease Neuroimaging Initiative*


\vspace{0.25 cm}

\small

$^1$Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA

$^2$Department of Neurobiology and Behavior, University of California, Irvine, Irvine, CA

$^3$Department of Statistics, University of California, Irvine, Irvine, CA

$^4$Department of Radiology, University of Pennsylvania, Philadelphia, PA

\end{centering}

\vspace{1.0 cm}

\small

Corresponding author: \
Nicholas J. Tustison \
211 Qureshey Research Lab \
Irvine, CA  92697-3800 \
ntustison@virginia.edu \

\noindent\rule{4cm}{0.4pt}

\footnotesize

${\dagger}$ Currently employed by Biogen (Cambridge, MA).  

*Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf

\newpage

\normalsize

__Abstract__

Longitudinal studies of development and disease in the human brain
have motivated the acquisition of large neuroimaging data sets and the
concomitant development of robust methodological and statistical tools
for quantifying neurostructural changes.  Longitudinal-specific strategies
for acquisition and processing have potentially significant benefits including
more consistent estimates of intra-subject measurements while retaining predictive power.
In this work, we introduce the open-source Advanced Normalization Tools
(ANTs) \textcolor{blue}{registration-based} cortical thickness longitudinal processing pipeline and its application
on the first phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI-1)
comprising over 600 subjects with multiple time points from baseline to 36 months.
We demonstrate \textcolor{black}{in these data} that the single-subject template
construction and native subject-space processing advantageously localizes data
transformations and mitigates interpolation artifacts \textcolor{black}{which results
in a simultaneous minimization of within-subject variability and maximization of
between-subject variability immediately estimable from a longitudinal mixed-effects
modeling strategy.}  It is further shown that optimizing these dual criteria
leads to greater scientific interpretability in terms of tighter confidence intervals
in calculated mean trends, smaller prediction intervals, and \textcolor{black}{narrower}
confidence intervals for determining cross-sectional effects.  \textcolor{black}{These concepts are
first illustrated and explored in the entorhinal cortex.  This
evaluation strategy is then extended to the entire cortex, as defined by the
Desikan-Killiany-Tourville labeling protocol, where comparisons are made with
the popular cross-sectional and longitudinal FreeSurfer processing streams.}

_Keywords:_  Advanced Normalization Tools, \textcolor{black}{entorhinal cortex},
\textcolor{black}{FreeSurfer}, interpolation,
\textcolor{black}{longitudinal mixed-effects}, longitudinal processing

\clearpage

# Introduction

Quantification of brain morphology significantly facilitates the
investigation of a wide range of neurological conditions with structural
correlates, including neurodegenerative conditions such as Alzheimer’s
disease [@du2007;@dickerson2009].  Essential for thickness quantification are the
many computational techniques which have been developed
to provide accurate measurements of the cerebral cortex.
These include various mesh-based
(e.g., [@macdonald2000;@magnotta1999;@kim2005]) and
volumetric techniques
(e.g., [@zeng1999;@jones2000;@das2009;@clement-vachet2011]).
Of noted significance, and representing the former,
is the well-known and highly utilized FreeSurfer
software package [@dale1999;@fischl1999;@fischl2000;@fischl2002;@fischl2004].


<!--
Quantification of brain morphology significantly facilitates the investigation of
a wide range of neurological conditions with structural correlates (e.g.,
Alzheimer's disease and frontotemporal dementia [@du2007;@dickerson2009],
Parkinson's disease [@jubault2011],
Williams syndrome [@thompson2005],
multiple sclerosis [@ramasamy2009],
autism [@chung2005,@hardan2006],
migraines [@dasilva2007],
chronic smoking [@kuhn2010],
alcoholism [@fortier2011],
cocaine addiction [@makris2008],
schizophrenia [@nesvag2008],
bipolar disorder [@lyoo2006],
autism [@chung2005,@hardan2006],
marijuana use in adolescents [@Jacobus:2015aa],
Tourette syndrome in children [@sowell2008],
scoliosis in female adolescents [@wang2012],
heart failure [@Kumar:2015aa],
early-onset blindness [@jiang2009],
chronic pancreatitis [@frokjaer2012],
obsessive-compulsive disorder [@shin2007],
ADHD [@almeida-montes2012],
obesity [@raji2010],
heritable [@peterson2009] and elderly [@ballmaier2004] depression,
age [@kochunov2011],
gender [@luders2006a],
untreated male-to-female transsexuality [@luders2012],
handedness
[@luders2006,amunts2007],
intelligence [@shaw2006],
athletic ability [@wei2011],
meditative practices [@lazar2005],
musical ability [@bermudez2009;@foster2010],
musical instrument playing [@Hudziak:2014aa],
tendency toward criminality [@raine2011],
childhood sexual abuse in adult females [@heim2013],
and Tetris-playing ability in female adolescents [@haier2009]).
-->

In inferring developmental processes, many studies employ
cross-sectional population sampling strategies despite the potential for
confounding effects [@Kraemer:2000aa].  Large-scale studies involving longitudinal
image acquisition of a targeted subject population, such as the Alzheimer's Disease
Neuroimaging Initiative (ADNI) [@Weiner2012],
are designed to mitigate some of the relevant statistical issues.  Analogously, much
research has been devoted to exploring methodologies for properly exploiting such
studies and avoiding various forms of processing bias [@Reuter:2012aa].  For example,
FSL's SIENA (Structural Image Evaluation, using Normalization, of Atrophy) framework
[@Smith:2002aa] for detecting atrophy between longitudinal image pairs avoids a specific type
of processing bias by transforming the images to a midspace position between the two
time points.
As the authors point out "[i]n this way both images are subjected to a similar degree
of interpolation-related blurring."  Consequences of this "interpolation-related
blurring" were formally analyzed in [@Yushkevich:2010aa] in the context of
hippocampal volumetric change where it was shown that interpolation-induced
artifacts can artificially create and/or inflate effect size [@Thompson:2011aa].  These insights
and others have since been used for making specific recommendations with respect to longitudinal
image data processing [@Avants:2010ab;@Fox:2011aa;@Reuter:2012aa;@Hua:2013aa].

In [@Reuter:2011aa;@Reuter:2012aa], the authors
motivated the design and implementation of the longitudinal FreeSurfer variant
inspired by these earlier insights and the overarching general principle
of "treat[ing] all time points exactly the same."  It has
since been augmented by integrated linear mixed effects modeling capabilities
[@Bernal-Rusiel:2013aa] and has been used in a variety of studies including pediatric
cortical development [@Wierenga:2014aa], differential development in Alzheimer's
disease and fronto-temporal dementia [@Landin-Romero:2016aa], and fatigue in the
context of multiple sclerosis [@Nourbakhsh:2016aa].

We introduced the Advanced Normalization Tools (ANTs)  cortical
thickness pipeline in [@Tustison:2014ab] which leverages various pre-processing, registration, segmentation,
and other image analysis tools that members of the ANTs and Insight Toolkit (ITK)
open-source communities have developed over the years and disseminated publicly [@ants].
This proposed ANTs-based pipeline has since been directed at a variety of neuroimaging
research topics including mild cognitive impairment and depression [@Fujishima:2014aa],
short term memory in mild cognitive impairment [@Das:2016aa], and aphasia [@Olm:2016aa].
Other authors extended the general framework to non-human studies [@PaganiDamianoGalbuseraEtAl2016;@MajkaChaplinYuEtAl2016].

In this work, we introduce the longitudinal version of the ANTs \textcolor{blue}{registration-based} 
cortical thickness
pipeline and demonstrate its utility on the publicly available ADNI-1 data set.
In addition, we demonstrate that certain longitudinal processing choices have significant impact
on measurement quality in terms of within-subject and between-subject variances which,
in turn, heavily impacts the scientific interpretability of results.  Similar to
\textcolor{black}{previously outlined} research illustrating the negative impact of
interpolation effects on study results,
we show that the common practice of reorienting individual time point images to a
single-subject template for unbiased processing induces interpolation artifacts
which guides processing choices for the proposed ANTs longitudinal pipeline.
These choices for the ADNI-1 data produce tighter confidence intervals
in calculated mean trends, smaller prediction intervals,
and less varied confidence/credible intervals for discerning cross-sectional effects.

<!--
To explore these findings in a more clinically oriented context, we also
use a machine learning-based training/prediction paradigm which demonstrates that
the recommended longitudinal processing approach leads to improved predictive diagnostic
accuracy over the alternative strategies, including cross-sectional processing.
-->


# Methods and materials

## ADNI-1 imaging data


The strict protocol design, large-scale recruitment, and public availability of
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) makes it
an ideal data set for evaluating the ANTs longitudinal cortical thickness pipeline.
An MP-RAGE [@Mugler:1990aa] sequence for 1.5 and 3.0 T was used to collect the data
at the scan sites.  Specific acquisition parameters for 1.5 T and 3.0 T magnets
are given in Table 1 of [@Jack:2008aa].  As proposed, collection goals were
200 elderly cognitively normal subjects collected at 0, 6, 12, 24, and 36 months;
400 MCI subjects at risk for AD conversion at 0, 6, 12, 18, 24, and 36 months; and
200 AD subjects at 0, 6, 12, and 24 months.  

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/demoPlot.png}
\caption{Demographic breakdown of the number of ADNI-1 subjects by diagnosis i.e., normal,
mild cognitive impairment (MCI), late mild cognitive impairment (LMCI),
and Alzheimer's disease (AD).  Within each panel we plot the number of subjects
(by gender) per visit---baseline (``bl'') and $n$ months (``m$n$'').}
\label{fig:demographics}
\end{figure}


The ADNI-1 data was downloaded in May of 2014 and first processed using
the ANTs cross-sectional cortical thickness pipeline [@Tustison:2014ab]
(4399 total images).  Data was then processed using two variants of the ANTs longitudinal
stream (described in the next section).  In the final set of csv files (which we have
made publicly available in the github repository associated with this work [@crossLong]),
we only included
time points for which clinical scores (e.g., MMSE) were available.  In total,
we included 186 elderly cognitive normals, 178 MCI subjects, 128 LMCI subjects,
and 123 AD subjects \textcolor{black}{with one or more follow-up image acquisition appointments}.
Further breakdown of demographic information is given
in Figures \ref{fig:demographics} and \ref{fig:mmse} to provide additional perspective
on the data used for this work.

<!--

```{r mycheck}
library(ADNIMERGE)
a1 = adnimerge[  drop(adnimerge$ORIGPROT == "ADNI1"), ]
table( a1$VISCODE[  a1$DX == "NL"] )
table( a1$VISCODE[  a1$DX == "MCI"] )
table( a1$VISCODE )
```

-->

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/demoPlot2.png}
\caption{Age vs. Mini-mental examination (MMSE) scores for the ADNI-1 subjects by diagnosis
 providing additional demographic characterization for the subjects processed for this study.}
\label{fig:mmse}
\end{figure}

## ANTs cortical thickness


### Cross-sectional processing

A thorough discussion of the ANTs cross-sectional thickness estimation framework
was previously discussed in [@Tustison:2014ab].  As a brief review, given a T1-weighted brain MR image,
processing comprises the following  major steps (cf Figure 1 of [@Tustison:2014ab]):

1. N4 bias correction [@Tustison:2010ac],
2. brain extraction [@avants2010a],
3. Atropos $n$-tissue segmentation [@Avants:2011aa], and
4. \textcolor{blue}{registration-based} cortical thickness estimation [@das2009].

ROI-based quantification is achieved \textcolor{black}{through joint label fusion} [@Wang:2013ab] \textcolor{black}{of
the cortex coupled with the MindBoggle-101 data.  These data use the Desikan–Killiany–Tourville (DKT)
labeling protocol} [@Klein:2012aa] \textcolor{black}{to parcellate each cortical hemisphere into 31 anatomical regions}
(cf Table \ref{table:dkt_labels}).
This pipeline has since been enhanced by the implementation [@Tustison:2016aa] of a patch-based
denoising algorithm [@Manjon:2010aa] as an optional preprocessing step and multi-modal
integration capabilities (e.g., joint T1- and T2-weighted image processing).  

\input{dktRegions.tex}

For evaluation, voxelwise regional thickness statistics were summarized based on the DKT
parcellation scheme.  Test-retest error measurements were presented
from a 20 cohort subset of both the OASIS [@oasis] and MMRR [@landman2011] data sets
and compared with the
corresponding FreeSurfer thickness values.    Further evaluation employed a training/prediction
paradigm whereby DKT regional cortical thickness values generated from 1205
images taken from four publicly available data sets (i.e., IXI [@ixi], MMRR,
NKI [@nki], and OASIS) were used to predict age and gender using linear and
random forest [@breiman2001] models.
The resulting regional statistics (including cortical thickness, surface area [@Lehmann:2012aa],
volumes, and Jacobian determinant values) were made available online [@kapowski].  These include the
corresponding FreeSurfer measurements which are also publicly available for research
inquiries (e.g., [@Hasan:2016aa]).
Since publication, this framework has been used in a number of studies
(e.g., [@Price:2015aa;@Wisse:2015aa;@Betancourt:2015aa]).


### Unbiased longitudinal processing

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/longitudinalPipeline.png}
\caption{Diagrammatic illustration of the ANTs longitudinal cortical thickness pipeline
for a single subject with $N$ time points.  From the $N$ original T1-weighted
images (left column, yellow panel) and the group template and priors (bottom row,
green panel), the single-subject template (SST) and auxiliary prior images
are created (center, blue panel).  These subject-specific template and other
auxiliary images are used to generate the individual time-point cortical
thickness maps, in the individual time point's native space (denoted as
``ANTs Native'' in the text).  Optionally, one can
rigidly transform the time-point images prior to segmentation and cortical thickness
estimation (right column, red panel).  This alternative processing scheme is referred
to as ``ANTs SST''.  For regional thickness values, regional labels
are propagated to each image using a given atlas set \textcolor{black}{(with cortical labels)
and joint label fusion.}}
\label{fig:pipeline}
\end{figure}

Given certain practical limitations (e.g., subject recruitment and retainment),
as mentioned earlier, many researchers employ cross-sectional acquisition and
processing strategies for studying developmental phenomena.  Longitudinal
studies, on the other hand, can significantly reduce inter-subject measurement variability.
The ANTs longitudinal cortical thickness pipeline extends the ANTs cortical
thickness pipeline for longitudinal studies which takes into account various
bias issues previously discussed in the literature
[@Yushkevich:2010aa;@Reuter:2011aa;@Reuter:2012aa].

Given $N$ time-point T1-weighted MR images (and, possibly, other modalities)
and representative images to create a population-specific template and
related images, the longitudinal pipeline consists of the following steps:

1. (Offline):  Creation of the group template and corresponding prior probability images.
2. Creation of the unbiased single-subject template (SST).
3. Application of the ANTs cross-sectional \textcolor{blue}{cortical thickness} pipeline [@Tustison:2014ab] 
   to the SST \textcolor{blue}{cwith the group template and priors as input.}
4. Creation of the SST prior probability maps.
5. (Optional):  Rigid transformation of each individual time point to the SST.
6. Application of the ANTs cross-sectional \textcolor{blue}{cortical thickness} 
   pipeline [@Tustison:2014ab], with the SST as the reference template, to each 
   individual time-point image.  \textcolor{blue}{Input includes the SST and 
   the corresponding spatial priors made in Step 3.}
7. Joint label fusion to determine the cortical ROIs for analysis.

An overview of these steps is provided in Figure \ref{fig:pipeline} which we describe
in greater detail below.  

<!--
One of the most significant findings presented below
is that the common step of transforming each individual
time point to the SST is suboptimal in that the corresponding interpolation
effects decrease the quality of cortical thickness measurements over
processing in native space.  
-->

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/adniTemplate2.png}
\caption{Top row:  Canonical views of the template created from 52 cognitively normal subjects
of the ADNI-1 database.  The prior probability mask for the whole brain (middle row)
and the six tissue priors (bottom row) are used to ``seed'' each single-subject template for creation of
a probabilistic brain mask and probabilistic tissues priors during longitudinal
processing.}
\label{fig:template}
\end{figure}

__ADNI group template, brain mask, and tissue priors.__  Prior to any individual subject processing, the group
template is constructed from representative population data [@Avants:2010aa].  For the ADNI-1 processing
described in this work, we created a population-specific template from 52 cognitively normal ADNI-1
subjects.  Corresponding brain and tissue prior probability maps for the CSF, gray matter,
white matter,  deep gray matter, brain stem, and cerebellum were created as described
in [@Tustison:2014ab].  A brief overview of this process is also provided in the  section
concerning creation of the single-subject template.
Canonical views of the ADNI-1 template and corresponding auxiliary images are given
in Figure \ref{fig:template}.



__Single-subject template, brain mask, and tissue priors.__
With the ADNI-1 group template and prior probability images,
each subject undergoes identical processing.  First, an average shape and intensity single
subject template (SST) is created from all time-point images  using the
same protocol [@Avants:2010aa] used to produce the ADNI-1 group template.
Next, six probabilistic tissue maps (cerebrospinal
fluid (CSF), gray matter (GM), white matter (WM), deep gray matter (striatum + thalamus),
brain stem, and cerebellum) are generated in the space of the SST.  This requires processing
the SST through two parallel workflows.  First,
the SST proceeds through the standard cross-sectional ANTs cortical thickness pipeline which generates
a brain extraction mask and the CSF tissue probability map, $P_{Seg}(CSF)$.  Second, using
a data set of 20 atlases from the OASIS data set that have been expertly annotated
\textcolor{black}{and made publicly available}
[@Klein:2012aa], a multi-atlas joint label fusion step (JLF) [@Wang:2013ab] is performed
to create individualized probability
maps for all six tissue types.  Five of the JLF probabilistic tissue
estimates (GM, WM, deep GM, brain stem, and cerebellum) and the JLF CSF estimate,
$P_{JLF}(CSF)$,
are used as the SST prior probabilities after smoothing with a Gaussian kernel
(isotropic, $\sigma = 1 mm$) whereas the CSF SST tissue probability
is derived as a combination of the JLF and segmentation CSF estimates, i.e.,
$P(CSF) = \max\left( P_{Seg}(CSF), P_{JLF}(CSF) \right)$, also smoothed
with the same Gaussian kernel.  Finally, $P(CSF)$ is subtracted out from the other
five tissue probability maps.  The final version of the SST and auxiliary images enable
unbiased, \textcolor{blue}{non-linear} mappings to the group template, subject-specific 
tissue segmentations, region of interest volumes and
cortical thickness maps for each time point of the original longitudinal image series.

__Individual time point processing.__ The first step for subject-wise
processing involves creating the SST
from all the time points for that individual [@Avants:2010aa].  For the cross-sectional ANTs processing,
the group template and auxiliary images are used to perform tasks such as individual
brain extraction and $n$-tissue segmentation prior to cortical thickness estimation [@Tustison:2014ab].
However, in the longitudinal variant, the SST serves this purpose.  We thus \textcolor{blue}{deformably}
map the SST and its priors to the native space of each time point where individual-level segmentation and cortical thickness is
estimated.  Note that this unbiased longitudinal pipeline is completely agnostic concerning ordering of
the input time-point images, i.e., we "treat all time points exactly the same."
\textcolor{blue}{Based on outcomes involving previously processed data sets 
(including ADNI-2), we chose to employ the denoising algorithm} [@Manjon:2010aa]
\textcolor{blue}{for all ANTs-based processing.}


During the initial development of this work, it was thought that an option allowing
for rotation of the
individual time points to the SST would be of benefit, similar to FreeSurfer, in reducing variability,
minimizing or eliminating possible orientation bias, and \textcolor{black}{possibly} permitting a 4-D
segmentation given that the underlying Atropos segmentation implementation is dimensionality-agnostic
[@Avants:2011aa].  Regarding the 4-D brain segmentation, any possible benefit is potentially
outweighed by the occurrence of "over-regularization" [@Reuter:2012aa] whereby
smoothing across time reduces detection ability of large time-point changes.
Additionally, it is less than straightforward to accommodate irregular temporal sampling
such as the acquisition schedule of the ADNI-1 protocol.  

In the FreeSurfer longitudinal stream, each time-point image is processed using
the FreeSurfer cross-sectional stream.  The resulting processed data from all time points
is then used to create a mean, or median, single-subject template.  Following
template creation, each time-point image is rigidly transformed to the template space where
it undergoes further processing (e.g., white and pial surface deformation).  This
reorientation to the template space "further reduce[s] variability" and permits an
"implicit vertex correspondence" across all time points [@Reuter:2012aa].
The ANTs longitudinal workflow shares some common aspects of its FreeSurfer
analog but differs in others as outlined above.  


<!--

However, during the course of this work we discovered that reorienting each time point image to the SST has
significant detrimental measurement effects in which interpolation bias induces
artificial anatomical changes and these changes correlate significantly with specific
regions.  Since we are measuring the thickness of the highly convoluted cortex
involving measurements on the order of $2-8 mm$ from images with $\sim 1 mm^3$
voxels, these artificial changes significantly effect clinically related
criteria of measurement quality such as confidence intervals and predictability.
This is discussed further in the following sections.

-->

__Joint label fusion and pseudo-geodesic for large cohort labeling.__  Cortical
thickness ROI-based analyses are performed using joint label fusion [@Wang:2013ab]
and whatever cortical parcellation scheme is deemed appropriate for the specific
study.  The brute force application of the joint label fusion algorithm would
require $N$ pairwise \textcolor{blue}{non-linear} registrations for each time-point image where $N$ is the
number of atlases used.  This would require a significant computational cost for
a relatively large study such as ADNI.  Instead, we use the "pseudo-geodesic" approach
for mapping atlases to individual time point images (e.g., [@Tustison:2015aa]).  The transformations between
the atlas and the group template are computed offline.  With that set of 
\textcolor{blue}{non-linear} transforms,
we are able to concatenate a set of existing transforms from each atlas through
the group template, to the SST, and finally to each individual time point \textcolor{black}{for
estimating regional labels for each image.}

## Statistical evaluation

\textcolor{black}{Based on the above ANTs pipeline descriptions, there are three
major variants for cortical thickness processing of longitudinal data.} We
denote these alternatives as:

* __ANTs Cross-sectional__ (or __ANTs Cross__). Process each subject's time point independently using the
  cross-sectional pipeline originally described in [@Tustison:2014ab].
* __ANTs Longitudinal-SST__ (or __ANTs SST__).  Rigidly transform each subject to the SST and then
   segment and estimate cortical thickness in the space of the SST.
* __ANTs Longitudinal-native__ (or __ANTs Native__).  Segment and estimate cortical thickness in the native space.

\textcolor{black}{For completeness, we also include a comparison with both the cross-section and
longitudinal FreeSurfer v5.3 streams respectively denoted as ``FreeSurfer Cross-sectional'' (or
``FS Cross'') and ``FreeSurfer Longitudinal'' (or ``FS Long'').}

\textcolor{black}{Possible evaluation strategies could employ manual measurements in
the histological} [@rosas2002] \textcolor{black}{or virtual} [@kuperberg2003]
\textcolor{black}{domains but would require an inordinate labor effort for collection
to be comparable with the size of data sets currently analyzed.  Other quantitative measures
representing ``reliability'', ``reproducibility'', or, more generally, ``precision''
can also be used to characterize such tools.  For example,} [@jovicich2013]
\textcolor{black}{used FreeSurfer cortical thickness measurements across image
acquisition sessions to demonstrate improved reproducibility with the longitudinal
stream over the cross-sectional stream.  In} [@Klein:2017aa] \textcolor{black}{
comparisons for ANTs, FreeSurfer, and the proposed method were made using
the range of measurements and their correspondence to values published in the
literature.  However, none of these precision-type measurements, per se,
indicate the utility of a pipeline-specific cortical thickness value as a potential biomarker.  
For example, Figure 8} in [@Tustison:2014ab] \textcolor{black}{confirms what was found in} [@Klein:2017aa]
\textcolor{black}{which is that the range of ANTs cortical thickness values for a particular region
exceeds those of FreeSurfer.  However, for the same data, the demographic predictive
capabilities of the former was superior to that of the latter.  Thus, better
assessment strategies are necessary for determining clinical utility.  For example,
the intra-class correlation (ICC) coefficient used
in} [@Tustison:2014ab] \textcolor{black}{demonstrated similarity in both ANTs and FreeSurfer
for repeated acquisitions despite the variance discrepancy between both sets of measurements.  
This is understood with the realization that the ICC takes into account both inter-observer
and intra-observer variability.}

### Regional within-subject and between-subject variance

\textcolor{blue}{A summary measure related to the ICC statistic} [@verbeke2009linear]
\textcolor{blue}{is used to quantify the relative performance of these
cross-sectional and longitudinal ANTs pipeline variants along with the
cross-sectional and longitudinal FreeSurfer streams.} Specifically, we use longitudinal
mixed-effects (LME) modeling to quantify pipeline-specific between-subject
and within-subject variabilities with the intuition that comparative
performance is determined by maximizing the ratio between the former
and the latter.}  Such a quantity implies greater within-subject reproducibility
while distinguishing between patient sub-populations
\textcolor{black}{(e.g., Alzheimer's disease diagnosis)}. As such this will
amount to higher precision when cortical thickness is used as a predictor
variable or model covariate in statistical analyses upstream. This criterion is
immediately estimable from the LME model \eqref{eq::lme1} outlined below.

LME models comprise a well-established and widely used class of regression models
designed to estimate cross-sectional and longitudinal linear associations between
quantities while accounting for subject specific trends.  As such, these models
are useful for the analysis of longitudinally collected cohort data.
Indeed, [@Bernal-Rusiel:2013aa] provide an introduction to the mixed-effects methodology
in the context of longitudinal neuroimaging data and compare it empirically to
competing methods such as repeated measures ANOVA. For more complete treatments of
the subject matter, see [@verbeke2009linear] and [@fitzmaurice2012applied].
LME models are also useful for estimating and comparing within-subject and
between-subject variability after conditioning out systematic time trends in
longitudinally measured data.  In the context of the current investigation, by
fitting simple LME models to the data resulting from cross-sectional and longitudinal
processing techniques, we are able to quantify the relative performance of each approach
with respect to within-subject, between-subject, and total variability in a way that
[@reuter2012] hint at in their exposition of the longitudinal FreeSurfer stream.

As previously noted we observed a longitudinal sampling of cortical thickness measurements
from 62 separate regions of interest.  To assess the above variability-based criteria
while accounting for changes that may occur through the passage of time, we used a
Bayesian LME model for parameter estimation.  Let $Y^k_{ij}$ denote the $i^{th}$
individual's cortical thickness measurement corresponding to the $k^{th}$ region of
interest at the time point indexed by $j$.  Under the Bayesian paradigm we utilized a model of the form
\begin{gather}
\label{eq::lme1}
Y^k_{ij} \sim N(\alpha^k_i + \beta^k t,
\sigma_k^2) \\ \nonumber \alpha^k_i \sim N(\alpha^k_0, \tau^2_k) \qquad
\alpha^k_0, \beta^k \sim N(0,10)  \qquad \sigma_k,  \tau_k \sim
\mbox{Cauchy}^+ (0, 5)
\end{gather}
Specification of variance priors to half-Cauchy distributions reflects commonly accepted best
practice in the context of hierarchical models [@gelman2006prior]. In this model,
$\tau_k$ represents the between-subject standard deviation, and $\sigma_k$ represents
the within-subject standard deviation, conditional upon time.  For each region $k$,
the quantity of interest is thus the ratio
\begin{equation}\label{eq::var_rat}
r^k = \frac{\tau_k}{\sigma_k}, \ k = 1, \dots, 62 \, .
\end{equation}
This ratio is at the heart of classical statistical discrimination methods as it
features both in the ANOVA methodology and in Fisher's linear discriminant analysis.
These connections are important since the utility of cortical thickness as a biomarker
lies in the ability to discriminate between patient sub-populations with respect to
clinical outcomes.

In particular, [@seber2012linear] (Sections 9.6.2 and 9.6.5) demonstrate the role that randomness and measurement error in explanatory variables play in statistical inference.  When the explanatory variable is fixed but measured with error (as is plausible for cortical thickness measurements), the within-subject variance divided by the between subject variance is proportional to the bias of the estimated linear coefficient when the outcome of interest is regressed over the explanatory variable (Example 9.2).  In short, the larger the $r^k$, the less bias for future statistical analyses based upon the cortical thickness data.  When the explanatory variable is considered random and is measured with error (a useful conception even with structural data), this bias is expressed as attenuation of regression coefficient estimates to zero by a multiplicative factor $r^k/(1+r^k)$ (Example 9.3). Thus, larger $r^k$ means less less attenuation bias and hence more discriminative capacity. 

The posterior distribution of $r^k$ was summarized via the posterior
median where the posterior distributions were obtained using the Stan probabilistic
programming language [@carpenter2016stan].

<!--
For each processing method, we performed 62 independent, region-specific regressions.  In order to compare results between methods, we considered the quantities
\begin{equation}
\delta^k = r^k_l - r^k_c\ , \quad \mbox{and} \quad \delta^k_{norm} = \frac{r^k_l - r^k_c}{r^k_l + r^k_c} \ ,
\end{equation}
denoting the variance ratio for the longitudinal method minus that of the cross-sectional method and the normed difference between ratios, respectively. Since a large $r^k$ implies a higher between-subject to within-subject variability ratio, a positive estimate of $\delta^k$ that is large in magnitude implies that the longitudinal processing method is preferable to the cross-sectional method.  Conversely, a negative estimate that is large in magnitude implies that the cross-sectional processing method is preferable to the longitudinal method.
-->

<!-- ### Practical implications of the variance ratio: a case study -->

<!-- The entorhinal cortex (EC) is one of the earliest regions to exhibit tau pathology -->
<!-- in the Alzheimer’s brain and is one of the first regions to show signs of -->
<!-- neurodegenerative change [@Hyman:1984aa;@Braak:1991aa;@Van-Hoesen:1991aa;@Yassa:2014aa]. -->
<!-- In the ADNI sample, EC cortical thickness was the most powerful measure of -->
<!-- structural change both in MCI and AD subjects [@Holland:2009aa]. EC thinning -->
<!-- was also found to precede and predict hippocampal atrophy [@Desikan:2010aa] -->
<!-- and to predict conversion to AD with the greatest accuracy [@Ewers:2012aa]. -->
<!-- Thus, we chose the EC to be the target of an additional focused analysis to determine -->
<!-- the relative utility of the different ANTs pipelines for measuring thickness in this -->
<!-- particular region.  Our choice of EC for comparative performance assessment is -->
<!-- motivated both by its selective vulnerability to neurodegenerative processes -->
<!-- as well as the difficulty of image segmentation in that particular region. -->

<!-- As a further assessment of utility as a biomarker, we used LME models and cortical -->
<!-- thickness measurements of the EC to demonstrate how these variability criteria -->
<!-- relate to potential scientific analyses. First, we used model \eqref{eq::lme1} -->
<!-- to show that a greater ratio of between-subject to within-subject variability -->
<!-- results in tighter confidence and credible intervals on the slope parameter $\beta$. -->
<!-- This result indicates more confidence with respect to mean trends over time that are of -->
<!-- common interest when comparing sub-populations of patients. Second, we showed that -->
<!-- smaller within-subject variability corresponds to smaller prediction intervals when -->
<!-- predicting a subject's cortical thickness levels at future visits. This is important -->
<!-- when considering regional cortical thickness measures as candidate biomarkers.   -->
<!-- Third, we use a simple linear regression model to compare the relationship between -->
<!-- total variance and uncertainty with respect to cross-sectional effects.  To do so, -->
<!-- we regress baseline cortical thickness in the entorhinal cortex (EC) over baseline -->
<!-- AD diagnostic status: -->
<!-- \begin{equation} \label{eq::slr} -->
<!-- ECCT_i = \beta_0 + \beta_1 AD_i + \epsilon_i \ . -->
<!-- \end{equation} -->
<!-- In general, lower total variability corresponds to tighter confidence/credible intervals -->
<!-- for cross-sectional covariate effects, and hence higher certainty when evaluating linear -->
<!-- associations between quantities such as cortical thickness and AD status.  If -->
<!-- total variability is similar across processing methods, we would expect to see -->
<!-- credible intervals of roughly the same size. -->

<!--
_Diagnostic prediction via extreme gradient boosting_

As mentioned earlier, an important component of our previously reported cross-sectional comparative
evaluation [@Tustison:2014ab] incorporated a statistical modeling approach for predicting basic
subject demographics (i.e., age and gender) from the summary DKT regional cortical thickness values.
We used a similar evaluation strategy in this work with the ADNI-1 data.  We built
statistical models from cortical thickness values for predicting the ADNI-specified diagnosis.
However, instead of using the regional cortical thickness values for all time points directly,
we used the longitudinal cortical thickness values for each DKT region of each subject to compute
a subject-specific, region-specific rate-of-thickness-change measurement generated from simple
linear regression of the available data.
Thus, for each subject, we calculated a single set of 62 slope coefficients,
$\mathbf{\zeta} = \{\zeta_1,\zeta_2,\dots,\zeta_{62}\}$,
and a single diagnosis of cognitively normal, MCI, LMCI, or AD.  Note that
the diagnosis for each subject did not change over the course of
the image acquisition schedule permitting this particular evaluative strategy.
This classification scenario yields the following model:
\begin{equation} \label{eq::xgboost_model}
DIAGNOSIS \sim \sum_{k=1}^{62} \zeta_k
\end{equation}  
The motivating idea is that regional thinning is accelerated in some regions versus
others which should be reflected in the assigned diagnostic category [@Holland:2009aa;@Desikan:2010aa].

For model construction, we used extreme gradient boosting, which is a well-performing,
out-of-the-box classifier implemented in the XGBoost [@xgboost]
package for the R project.  Although there are many options available for classification, we chose
this particular technique due to our recent interest in other projects
based on its various successes in other fields.[^1002]  An additional advantage is that XGBoost provides
the "gain" quantity which describes "the improvement in accuracy brought by a feature to the branches
[of the tree or iteration] it is on [@xgboost]." These can be reviewed for
clinical plausibility of the results.


[^1002]: http://stat-computing.org/awards/jmc/winners.html

-->

# Results


Based on the evaluation design described in the previous section, we compare
the performance of the five processing approaches \textcolor{black}{(FS Cross, FS Long, ANTs Cross,
 ANTs SST, and ANTs native)} as applied to the ADNI-1 data.  Specifically, we demonstrate how the variance ratio
defined in Equation \eqref{eq::var_rat} illustrates ways in which different aspects of
variability affect confidence in prediction and estimation \textcolor{black}{for these different pipelines}.

<!--

\textcolor{black}{
Shown in Figure \ref{fig:spaghetti} are sample spaghetti plots derived from the
longitudinal cortical thickness values of the left EC and neighboring structures.
Note that the remaining plots for all regions and the R scripts to generate the
plots are available in the associated github repository} [@crossLong]\textcolor{black}{.  These
are presented in the context of the earlier discussion concerning
the data variability differences between ANTs and FreeSurfer cross-sectional
regional thickness values.   
}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/LeftThicknessSpaghettiPlots.png}
\caption{\textcolor{black}{Sample spaghetti plots for the longitudinal thickness values estimated
from each of the five pipelines.   Shown are the EC and neighboring structures
in the left hemisphere.  These demonstrate the characteristic difference in data
``spread'' in the associated ANTs and FreeSurfer-based measurements.}
}
\label{fig:spaghetti}
\end{figure}

-->

### Cortical within-subject and between-subject thickness variability

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{../Figures/variance.ratio_FINALX.png}
\caption{95\% credible intervals of the region-specific variance ratios
$r^k=\tau_k/\sigma_k$ are presented for each processing method.  The ANTs
Longitudinal-native method dominates the others \textcolor{black}{across the majority of
regions}: its point estimates (posterior medians) are greater than those of the other
processing methods \textcolor{black}{except for the left and right EC values in
FreeSurfer Long (although there is significant overlap in the credible intervals
in those regions)}.  
These results also suggest that \textcolor{black}{longitudinal processing is to be
preferred for both packages.}}
\label{fig:ratios}
\end{figure}

Our first evaluation strategy was to use LME models to quantify the between-subject
and within-subject variance with the expectation that maximizing the former while minimizing
the latter optimizes measurement quality in terms of prediction and confidence intervals.
Figure \ref{fig:ratios} provides the resulting 95\% credible intervals
for the distributions of region-specific variance ratios $r^k = \tau_k / \sigma_k$
for each of the five pipelines.  The superior method is designated by larger variance ratios and has
the greater discriminative capacity for the data corresponding to that processing method.

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{../Figures/allData_FINALX.png}
\caption{Box plots showing the distribution of the within-subject variability,
between subject variability, and ratio of the between-subject variability and
within-subject variability for each of the 62 DKT regions.  Note that the
``better'' measurement maximizes this latter ratio.}
\label{fig:variance_boxplots}
\end{figure}

ANTs Native has the highest ratio variance across most of the 62 regions over the
other methods.  It rarely overlaps with ANTs SST and never with
ANTs Cross.  \textcolor{black}{
In contrast to the majority of FreeSurfer regional ratio variances (from
both FS Cross and FS Long) which are smaller than those of the ANTs pipelines,
FS Long has larger ratio values for the EC region with the only overlap in the
credible intervals with ANTs Native.}

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{../Figures/medianRatios3D.png}
\caption{3-D volumetric rendering of the regional variance ratio values on the generated ADNI template.
The higher variance ratios indicate greater between-subject to within-subject variability.}
\label{fig:brain_variance}
\end{figure}

The plot in
Figure \ref{fig:variance_boxplots} shows a relative summary of all the regional quantities
for all three variance measurements (within-subject, between-subject, and variance
ratio) via box plots.  These relative distributions show
that both between-subject and within-subject quantities contribute to the disparities
in the ratio evaluation metric.  Finally, we overlay the variance ratio values on the
corresponding regions of a 3-D rendering of the ADNI template (Figure \ref{fig:brain_variance})
to provide an additional visual comparison between the methods.
Therefore, Figures \ref{fig:ratios}, \ref{fig:variance_boxplots}, and \ref{fig:brain_variance}
may be considered as evidence for ANTs Native
providing higher quality data than
those provided by the other methods.   

<!-- ### Case study: entorhinal cortical thickness variability -->

<!-- \input{ecStatisticalResults.tex} -->


<!-- Here we present results from the entorhinal cortical thickness case study (Section 2.3.2). -->
<!-- We demonstrate the way in which different arrangements of within-subject and -->
<!-- between-subject variability influence statistical inference and prediction.   -->
<!-- Data quality translates directly to quality of statistical results and the -->
<!-- scientific conclusions derived therefrom. Hence, data with good variance and -->
<!-- precision properties will benefit statistical analyses in multiple ways. To -->
<!-- demonstrate these benefits, we focus on data from the EC and -->
<!-- present three different aspects of variability and their statistical upshots. -->
<!-- Table \ref{table:res_tab} presents different aspects of model variability and shows -->
<!-- their relationships to uncertainty in prediction and estimation.  Model variability -->
<!-- is shown in terms of point estimates (posterior medians) for different functions -->
<!-- of the variance terms from Model \eqref{eq::lme1}.  Predictive and estimation -->
<!-- uncertainty takes the form of credible interval widths and predictive variance. -->
<!-- The larger these quantities, the more uncertainty, and hence the less definite -->
<!-- the scientific conclusions reached.  Both raw and normalized results are presented. -->
<!-- For each quantity, the cells corresponding to highest performance are colored green, -->
<!-- and those corresponding to worst performance are colored red. -->

<!-- On the left of Table \ref{table:res_tab}, the variance ratio is presented alongside -->
<!-- the width of the credible interval corresponding to the slope parameter $\beta$ -->
<!-- from Model \eqref{eq::lme1}. In general, a higher ratio of between-subject and -->
<!-- within-subject variances implies greater precision when estimating trends and -->
<!-- associations through time.  As expected from the previous results regarding the -->
<!-- ratio of between- and within-subject variability, ANTs Native -->
<!-- yields the smallest credible interval on the slope parameter. -->

<!-- In the middle of Table \ref{table:res_tab}, within-subject variability is presented -->
<!-- alongside predictive variance, i.e., the median for each subject-specific empirical -->
<!-- variance when predicting EC thickness 6 months out from the last observation. -->
<!-- As might be expected these two quantities track closely to each other, since prediction -->
<!-- variability is an amalgam of within-subject variability and uncertainty in model -->
<!-- parameters.  Again, the ANTs Native method performs best whereas ANTs SST performs worst. -->

<!-- \begin{figure}[ht!] -->
<!-- \centering -->
<!-- \includegraphics[width=\textwidth]{../Figures/results_plot_WithScr.png} -->
<!-- \caption{Aspects of model variance are compared with -->
<!-- credible interval sizes and variance in predictions for the ANTs-based -->
<!-- pipelines. Values are normalized by the -->
<!-- largest quantity, and processing methods are distinguished by color and ordering. -->
<!-- On the left, the variance ratio $r = \tau / \sigma$ is compared to the width of -->
<!-- credible interval for the slope term of Model \eqref{eq::lme1}. In the middle, -->
<!-- within-subject variance, $\sigma^2$, is compared to predictive variance.  On the -->
<!-- right, total variance, $\sigma^2 + \tau^2$, is compared to width of credible interval -->
<!-- for the cross-sectional association of AD status with EC thickness.}\label{fig1} -->
<!-- \end{figure} -->


<!-- Finally, the right side of Table \ref{table:res_tab}, compares total variance to -->
<!-- the width of credible intervals pertaining to the cross-sectional association of -->
<!-- AD diagnosis and EC thickness as modeled in Equation \eqref{eq::slr}. -->
<!-- As total variance rises, so too does uncertainty in cross-sectional effects. -->
<!-- However, all three processing methods achieve roughly the same amount of total -->
<!-- variability, so no trend is visible.  It is interesting to observe that for this -->
<!-- particular example the lower bound of the second longitudinal is farther from the -->
<!-- null effect of zero when compared to the other two approaches. That is, despite -->
<!-- having marginally greater total variance, the distance from zero for the credible -->
<!-- interval corresponding to ANTs Native is 0.81, whereas the -->
<!-- distances for ANTs SST and ANTs Cross are 0.75 and -->
<!-- 0.70, respectively.  Figure \ref{fig1} displays the normalized results. -->



<!--

_Diagnostic prediction via extreme gradient boosting_

A clinically-based prediction strategy was performed to evaluate the quality
of the cortical thickness measurements produced by each method.  The rate of
thickness change determined over the set of subject imaging visits was used
as a feature set for predicting diagnosis.  The basic idea is that regional
thinning is accelerated in some regions versus others which should be reflected
in the assigned diagnostic category.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../Figures/accuracyBarPlot0.9.png}
\caption{Bar plot of the accuracy for each of the ANTs pipeline alternatives along with
the distribution summary produced from random guessing.  Accuracy was
 determined from the confusion matrices that were calculated for each of the
 1000 iterations.  Using Tukey multiple comparisons of means at a 95\% family-wise
 confidence level, the adjusted p-values were: Longitudinal-SST $-$ Cross-sectional $< 1e-4$,
 Longitudinal-native $-$ Cross-sectional $< 1e-6$, Longitudinal-native $-$ Longitudinal-SST $< 1e-6$.}
\label{fig:xgbDensity}
\end{figure}

Using the model described by Equation \eqref{eq::xgboost_model}, we compared
the classification capabilities of each of the three pipelines.  For each of
$N$ iterations, the data was randomly split 90/10 (i.e., 90\% training and 10\% testing)
and used to construct three diagnostic classification models from the training data,
one for each pipeline.  The testing data portion and the corresponding prediction
sets were used to construct confusion matrices from which diagnostic accuracy was
calculated.  Chosen XGBoost model parameters deviating from the default were:
number of trees = number of iterations ("nrounds") = 100 and
gradient step ("eta") = 0.3, based on parameter tuning on a small data subset over
all three pipelines.

\begin{figure}
\centering
\includegraphics[width=80mm]{../Figures/importanceCombinedLong20.9.png}
\caption{At each iteration of the extreme gradient
boosting optimization, the accuracy of the model is dependent on splitting
the current tree on a single feature to correct any remaining misclassifications.  
The improvement in accuracy derived from each feature split is quantified by the gain.
The gain measurements of the extreme gradient boosting models for the
ANTs longitudinal-native processing.  We average over all iterations and
each training/testing run to produce the mean value for each feature plotted above.
The bar and whiskers for each feature represents a single standard deviation
of the average gain over all the training/testing runs.
}
\label{fig:xgbGain}
\end{figure}

The resulting accuracy distribution summariess are plotted in Figure \ref{fig:xgbDensity} and
compared statistically using Tukey's range test which indicated increasing
performance Cross-sectional < Longitudinal-SST < Longitudinal-native.  These models
also provide means for assessing feature importance through the "gain" values.
The gain plot is given in Figure \ref{fig:xgbGain}.
The information offered by the gain plot is consistent with what we know about selective
vulnerability of brain regions to AD-related neurodegeneration [@dickerson2009].
For example, medial temporal regions such as the entorhinal and parahippocampal
cortices are near the top offering the most diagnostic information, whereas regions
such as paracentral cortices are near the bottom, offering very little diagnostic information.

-->

<!--

_Interpolation effects associated with Longitudinal-SST processing_

[\textcolor{black}{Nick is working on this section.  Might not be included.}]

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{../Figures/oasisCorrelationVolumeAndSurfaceArea.png}
\caption{The between-subject variability (left) and within-subject variability (right)
versus the volumetric (top) and surface area (bottom) measurements.
Note that each of these correlations are significant ($p < 0.001$)}
\label{fig:interp}
\end{figure}

The subset of regions where the Longitudinal-SST $<$ Cross-sectional in terms of
the variance ratio caused us to investigate this issue further.  Considering the
only difference between Longitudinal-SST and Longitudinal-native is the reorientation
to the SST, our exploration focused on the effects of interpolation. Specifically,
we suspected that interpolation artificially changes the anatomy (i.e., volume
and surface area) which results in additive  noise to the thickness measurements
and that this effect varies spatially.  To test this, we used the 20 DKT atlases
[@klein2012] which were sampled from the OASIS data set.  An optimal mean/shape
template [@Avants:2010aa] was created from this cohort to which each T1-weighted
image was rigidly registered.  Using the resulting rigid transform, the label
map of 62 regions was warped to the space of the template (similar in spirit to
the protocol characterizing Longitudinal-SST) using nearest neighbor
interpolation.  Calculation of the volumes (by summing up the volume of each
voxel) and surface areas (using a well-performing digitally-based surface estimator
[@Lehmann:2012aa]) for each of the 62 regions were calculated both before and after
transformation to the template.  We then computed the percent change for each
of the two anatomical measures and correlated those with the per subject variability
which is plotted in Figure \ref{fig:interp}.


-->

# Discussion

Herein we detailed the ANTs \textcolor{blue}{registration-based} longitudinal cortical thickness framework which
is designed to take advantage of longitudinal data acquisition protocols while
accounting for the various bias issues that have been associated with processing
such data.  Over 600 subjects from the well-known longitudinal ADNI-1 data set
with diagnoses distributed between cognitively normal, MCI, LMCI, and AD were
processed through the original ANTs cross-sectional framework [@Tustison:2014ab] and
two longitudinal variants.  One of the variants, ANTs SST, is
similar to the FreeSurfer longitudinal stream in that each time-point image is
reoriented to an unbiased single-subject template for subsequent processing.
ANTs Native, in contrast, estimates cortical thickness in the native
space while also using tissue prior probabilities generated from the SST.  

Comparative assessment utilized LME models to determine the between-subject to
within-subject variance ratios over the 62 regions of the brain defined by the
DKT parcellation scheme where higher values indicate greater discriminative
capacity.  In these terms, ANTs Native outperformed all other pipeline variants
including both the FreeSurfer longitudinal and cross-sectional streams.
Regional disparities between the ANTs Native and SST pipelines point to increases
in both between-subject and within-subject variances which might be due to interpolation
artifacts.  In other words, interpolation potentially has a systematic but regionally varying effect.
Investigation of this issue is the subject of future research.  

\textcolor{black}{
One very interesting finding was the superior performance of FS Long in the EC regions where
the variance ratios was slightly larger than those of ANTs Native where the credible intervals
have significant overlap.  Given the  small volume and indistinguishability from surrounding
structures, segmentation of the EC can be relatively difficult} [@price2010]\textcolor{black}{.
This segmentation complexity has led to EC-specific} [@Fischl:2009aa] \textcolor{black}{and related}
[@Augustinack:2013aa] \textcolor{black}{strategies for targeted regional processing.  
For this work, we wanted to avoid such tuning and simply employ off-the-shelf
input parameters and data.  Future work will explore refining input template priors
in these problematic regions for ANTs-based estimation of cortical thickness.
}  

Additional assessments included similar variance quantification in the EC
and diagnostic prediction using extreme gradient boosting models.  The
former evaluation was motivated by the prominence of the EC as a biomarker
in AD progression whereas the latter coupled a simplistic assumption of AD
progression with modern machine learning techniques similar in spirit to what
we did in our previous work [@Tustison:2014ab].  Both assessments supported the
findings of the first assessment in demonstrating the superiority of
Longitudinal-native.  These findings promote longitudinal analysis
considerations and motivates such techniques over cross-sectional methods
for longitudinal data despite the increase in computational costs.

The longitudinal thickness framework is available in script form
within the ANTs software library along with the requisite processing components
(cf Appendix).  All generated
data used for input, such as the ADNI template and tissue priors, are available
upon request.  As previously mentioned, we also make available the csv files
containing the regional thickness values for all three pipelines.

\clearpage



# Appendix

## Implementation overview

The script ``antsLongitudinalCorticalThickness.sh`` performs cortical thickness
estimation for a longitudinal image series from a single subject.  The following
principal steps are performed:

1. A single-subject template (SST) is created from all the time point images.
2. The tissue prior probability images are generated for the SST. These six tissues are
   label 1:  CSF, label 2: cortical gray matter, label 3: white matter, label 4: deep
   gray matter, label 5: brain stem, and label 6: cerebellum.  Prior probability creation
   involves the following steps:  

    1. The SST is passed through ``antsCorticalThicknes.sh``.
    2. The brain extraction posterior for the SST is created by smoothing the brain extraction
       mask created during 2a.
    3. If labeled atlases are not provided, we smooth the posteriors from 2.1 to create
       the SST segmentation priors, otherwise we use the ``antsJointFusion`` program
       to create a set of posteriors using the script ``antsCookTemplatePriors.sh``.

3. Using the SST + priors, each subject is processed through the ``antsCorticalThickness.sh``
   script.

A typical command line call is:
```
antsLongitudinalCorticalThickness.sh \
              -d ${imageDimension} \
              -e ${brainTemplate} \
              -m ${brainExtractionProbabilityMask} \
              -p ${brainSegmentationPriors}
              -o ${outputPrefix}
              ${anatomicalImages[@]}
```

## Input parameters

* ``imageDimension``:  dimensionality of the input images.  Can handle 2 or 3 dimensions.
* ``brainTemplate``:  the group template.  We have made several publicly available
along with the prior tissue and brain extraction images
(https://figshare.com/articles/ANTs_ANTsR_Brain_Templates/915436).
* ``brainExtractionProbabilityMask``:  prior probability image for the whole brain
corresponding to the ``brainTemplate``.
* ``brainSegmentationPriors``:  prior probability images for the six brain tissues
mentioned above.  These files are specified with the relevant labels, e.g., ``prior1.nii.gz``,
``prior2.nii.gz``, ``prior3.nii.gz``, ``prior4.nii.gz``, ``prior5.nii.gz``,
and ``prior6.nii.gz``.  The command line argument is specified in C-style formatting, e.g.,
``prior%d.nii.gz``.
* ``anatomicalImages``:  the time point images for a single subjects.
* other optional input parameters are available.  ``antsLongitudinalCorticalThickness -h``
  provides a listing of the full set of parameters, their descriptions, and other help information.

## Output

In the specified output directory, the following subdirectories are created:

* ``${outputPrefix}SST``
* ``${outputPrefix}${anatomicalImagesPrefix[0]}``
* ``${outputPrefix}${anatomicalImagesPrefix[1]}``
* ``${outputPrefix}${anatomicalImagesPrefix[2]}``
* ...

Each subdirectory contains the output of `antsCorticalThickness.sh` applied to the
corresponding image.  Output consists of the following files:

* `BrainExtractionMask`: Brain extraction mask in subject space.
* `BrainNormalizedToTemplate`: Extracted brain image normalized to the template space.
* `BrainSegmentation0N4`: Input to the segmentation algorithm. It is not brain extracted, but is bias-corrected. If multiple images are used for segmentation, there will be `BrainSegmentation1N4` and so on. The brain extracted version of this is `ExtractedBrain0N4`.
* `BrainSegmentation`: Segmentation image, one label per tissue class. The number of classes is determined by the input priors.
* `BrainSegmentationPosteriors1`: Posterior probability of class 1. A similar image is produced for all classes. The numbering scheme matches the input priors.
* `CorticalThickness`: Cortical thickness image in subject space.
* `CorticalThicknessNormalizedToTemplate`: Cortical thickness image in template space.
* `ExtractedBrain0N4`: Brain-extracted version of `BrainSegmentation0N4`.
* `SubjectToTemplate1Warp`, `SubjectToTemplate0GenericAffine.mat`:  Transforms to be used when warping images from the subject space to the template space.
* `SubjectToTemplateLogJacobian`: Log of the determinant of the Jacobian, quantifies volume changes in the subject to template warp.
* `TemplateToSubject0Warp`, `TemplateToSubject1GenericAffine.mat`: Transforms to be used when warping images from the template to the subject space.


In addition to these files, the SST subdirectory contains additional warps, suffixed "SubjectToGroupTemplateWarp.nii.gz" and "SubjectToTemplate0GenericAffine.mat", that can be used to warp each time point image to the group template. These are a combination of the subject to SST warp, and the SST to group template warp.  Also included are the SST brain and tissue prior probability images.  



\clearpage

\newpage

__Acknowledgments__

Additional support to N.T. and M.Y. provided by NIMH R01 MH102392 and NIA R21 AG049220, P50 AG16573.



Data collection and sharing for this project was funded by the Alzheimer's
Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant
U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012).
ADNI is funded by the National Institute on Aging, the National Institute of
Biomedical Imaging and Bioengineering, and through generous contributions from
the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery
Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb
Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli
Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated
company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer
Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical
Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale
Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis
Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda
Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes
of Health Research is providing funds to support ADNI clinical sites in Canada.
Private sector contributions are facilitated by the Foundation for the National
Institutes of Health (www.fnih.org). The grantee organization is the Northern
California Institute for Research and Education, and the study is coordinated
by the Alzheimer’s Therapeutic Research Institute at the University of Southern
California. ADNI data are disseminated by the Laboratory for Neuro Imaging at
the University of Southern California.

\newpage

# References
